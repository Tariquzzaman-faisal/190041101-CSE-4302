{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tzf101/190041101-CSE-4302/blob/master/utils_notebook/sr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TudFKonjbZRC"
      },
      "source": [
        "# Loading Libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzMyqYJ-MpZP"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOeEB1YSMr6S",
        "outputId": "ba8032b3-167d-41d1-ffcf-af7c98d10143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/MyDrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK7H47A1M0Iv",
        "outputId": "83335f95-7239-41a1-fb95-195998f098fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install rouge\n",
        "!pip install sacrebleu\n",
        "!pip install -U sentence-transformers\n",
        "!pip install bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBamt2QyM-rn"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "\n",
        "import sacrebleu\n",
        "from rouge import Rouge\n",
        "from sacrebleu import corpus_bleu\n",
        "from bert_score import score\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtDB5CFDbe-T"
      },
      "source": [
        "### Loading Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kry4FrZlNBrJ"
      },
      "outputs": [],
      "source": [
        "sbert_model = SentenceTransformer('l3cube-pune/bengali-sentence-bert-nli')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKUeTKEdNJSZ"
      },
      "source": [
        "### Score calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAR-9jrpNMK6"
      },
      "outputs": [],
      "source": [
        "def calculate_sbert_score(original, augmented):\n",
        "    emb1 = sbert_model.encode(original)\n",
        "    emb2 = sbert_model.encode(augmented)\n",
        "    cosine_scores = util.pytorch_cos_sim(emb1, emb2)\n",
        "    sbert_score = cosine_scores.item()\n",
        "    return sbert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9be5SZ92NOWF"
      },
      "outputs": [],
      "source": [
        "def calculate_scores(original, augmented):\n",
        "    # BLEU Score\n",
        "    reference = original\n",
        "    candidate = augmented\n",
        "    # bleu_score = sentence_bleu(reference, candidate)\n",
        "    bleu_score = [sacrebleu.corpus_bleu([aug], [[orig]]).score for aug, orig in zip(augmented, original)]\n",
        "    # BERTScore\n",
        "    P, R, F1 = score([augmented], [original], lang=\"bn\", rescale_with_baseline=True)\n",
        "\n",
        "    # SBERT Score with Cosine Similarity\n",
        "    emb1 = sbert_model.encode(original)\n",
        "    emb2 = sbert_model.encode(augmented)\n",
        "    cosine_scores = util.pytorch_cos_sim(emb1, emb2)\n",
        "    sbert_score = cosine_scores.item()\n",
        "\n",
        "    return bleu_score, F1.item(), sbert_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT_VrLx0NdNI"
      },
      "source": [
        "### SR function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS37qbvAeVx4"
      },
      "outputs": [],
      "source": [
        "!pip install bnlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4TpjxqphEX1"
      },
      "outputs": [],
      "source": [
        "!pip install bnlp-toolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuR2C5hxey96"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from random import shuffle\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4DAKpkDhMEE"
      },
      "outputs": [],
      "source": [
        "from bnlp import BengaliCorpus as corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buvlM9FLd9Ui"
      },
      "outputs": [],
      "source": [
        "from bnlp import BengaliWord2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQLXTbNnkKXN"
      },
      "outputs": [],
      "source": [
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    bwv = BengaliWord2Vec()\n",
        "    similar_words = bwv.get_most_similar_words(word, topn=10)\n",
        "    for word in similar_words:\n",
        "        synonyms.add(word[0])\n",
        "    if word in synonyms:\n",
        "        synonyms.remove(word)\n",
        "    return list(synonyms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ0KezAWvxmQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class SR():\n",
        "    def __init__(self, stopwords):\n",
        "        self.stopwords = stopwords\n",
        "\n",
        "    def augment(self, text, n, debug=False):\n",
        "        words = text.split()\n",
        "        new_words = words.copy()\n",
        "        random_word_list = list(set([word for word in words if word not in self.stopwords]))\n",
        "        random.shuffle(random_word_list)\n",
        "        num_replaced = 0\n",
        "        for random_word in random_word_list:\n",
        "            try:\n",
        "                synonyms = get_synonyms(random_word)\n",
        "                if len(synonyms) >= 1:\n",
        "                    synonym = random.choice(list(synonyms))\n",
        "                    new_words = [synonym if word == random_word else word for word in new_words]\n",
        "                    num_replaced += 1\n",
        "            except KeyError:\n",
        "                # Ignore words not present in the vocabulary\n",
        "                continue\n",
        "            if num_replaced >= n:\n",
        "                break\n",
        "\n",
        "        output = ' '.join(new_words)\n",
        "        if debug:\n",
        "            output += \"(sr)\"\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFoqvF4NTAWw"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8wyLXLphtwO"
      },
      "outputs": [],
      "source": [
        "stopwords = set(corpus.stopwords)\n",
        "sr = SR(stopwords)\n",
        "\n",
        "text = \"সঠিক তদন্ত করতে হবে। বিচারের আওতায় আনতে হবে যে এই কাজ টা করেছে।\"\n",
        "augmented_text = sr.augment(text, n=2)\n",
        "print(augmented_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT7mLqWyTFZX"
      },
      "source": [
        "### Apply SR on dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2ZOcUu3TC9I"
      },
      "outputs": [],
      "source": [
        "# Define a function to apply paraphrasing\n",
        "def apply_sr(row):\n",
        "    return sr.augment(row['original_sentence'], n=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V164XtxZTeWX"
      },
      "outputs": [],
      "source": [
        "def sr_and_evaluate_dataset(file_path, original_col_name, new_col_name):\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df.rename(columns={original_col_name: 'original_sentence'})\n",
        "\n",
        "    # Apply the text augmentation function\n",
        "    df[new_col_name] = df.apply(apply_sr, axis=1)\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    rouge = Rouge()\n",
        "    rouge_scores = rouge.get_scores(df[new_col_name], df[\"original_sentence\"])\n",
        "    rouge_df = pd.DataFrame([\n",
        "        {\n",
        "            'r1f1': score['rouge-1']['f'],\n",
        "            'r2f1': score['rouge-2']['f'],\n",
        "            'rlf1': score['rouge-l']['f'],\n",
        "        }\n",
        "        for score in rouge_scores\n",
        "    ])\n",
        "\n",
        "    # Compute SacreBLEU scores\n",
        "    sacrebleu_scores = [sacrebleu.corpus_bleu([aug], [[orig]]).score for aug, orig in zip(df[new_col_name], df[\"original_sentence\"])]\n",
        "    df[\"sacrebleu_score\"] = sacrebleu_scores\n",
        "\n",
        "    # SBERT Score with Cosine Similarity\n",
        "    df[\"sbert_score\"] = [calculate_sbert_score(orig, aug) for orig, aug in zip(df[\"original_sentence\"], df[new_col_name])]\n",
        "\n",
        "    # BERTScore\n",
        "    P, R, F1 = score(df[new_col_name], df[\"original_sentence\"], lang=\"en\", rescale_with_baseline=True)\n",
        "    df[\"bertscore_f1\"] = F1.tolist()\n",
        "\n",
        "    # Combine the dataframes\n",
        "    result_df = pd.concat([df, rouge_df], axis=1)\n",
        "    result_df[\"method\"] = \"sr2\"\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e9HfvGMTqMU"
      },
      "source": [
        "# Running on dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksQrWUGiTrtl"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/MyDrive/MyDrive/Research/Thesis: BDA/Main/evaluation/(old)Youtube/Datasets/yt_sentiment_train_10.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WAr2BptTyaV"
      },
      "outputs": [],
      "source": [
        "result_df = sr_and_evaluate_dataset(file_path, 'sentence1', 'augmented_sentence')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuiK-qrlT1yv"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxB1ZH7mT8Vv"
      },
      "source": [
        "### Saving augmented dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwyluE9QT6Tz"
      },
      "outputs": [],
      "source": [
        "result_df.to_csv('/content/MyDrive/MyDrive/Research/Thesis: BDA/Main/evaluation/(old)Youtube/Datasets/yt_sentiment_train_10_sr.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaScpNdfw2LN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqvo1auptL3FX6ZFB9VjWL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}